"""
è®­ç»ƒU-Netæ¨¡å‹ç”¨äºPASCAL VOC 2012è¯­ä¹‰åˆ†å‰²ä»»åŠ¡
ä¿®å¤ç‰ˆæœ¬ - æ·»åŠ IoUå’ŒDiceç³»æ•°è¯„ä¼°

ä½œè€…: Kayu J
æ—¥æœŸ: 2025å¹´9æœˆ16æ—¥
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from datetime import datetime
import time
import warnings
warnings.filterwarnings('ignore')

from unet_model import SimpleUNet, UNet
from voc_seg_dataset2 import VOCSegmentationDataset

# ==================== å‚æ•°è®¾ç½®åŒºåŸŸ ====================
DATA_DIR = "VOC2012"  # VOCæ•°æ®é›†è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„ï¼‰
BATCH_SIZE = 2                # æ‰¹æ¬¡å¤§å°
NUM_WORKERS = 0                 # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°

# æ¨¡å‹ç›¸å…³å‚æ•°
N_CLASSES = 21                    # ç±»åˆ«æ•°é‡

# è®­ç»ƒç›¸å…³å‚æ•°
EPOCHS = 120                      # è®­ç»ƒè½®æ•°ï¼ˆå¯¹ç…§å®éªŒæ”¹ä¸º120ï¼‰
LEARNING_RATE = 1e-4              # å­¦ä¹ ç‡ï¼ˆæé«˜åˆå§‹å­¦ä¹ ç‡ä»¥çªç ´ç“¶é¢ˆï¼‰
WEIGHT_DECAY = 1e-5               # æƒé‡è¡°å‡
LOSS_TYPE = "combined"            # æŸå¤±å‡½æ•°ç±»å‹ï¼ˆå¯¹ç…§ï¼š0.5*CE+0.5*Diceï¼‰

# ä¼˜åŒ–å™¨å‚æ•°
OPTIMIZER_TYPE = "adam"           # ä¼˜åŒ–å™¨
MODEL_TYPE = "unet"               # æ¨¡å‹ç±»å‹ï¼š"simple" æˆ– "unet"
SCHEDULER_TYPE = "multistep"  # å­¦ä¹ ç‡è°ƒåº¦ï¼š"cosine" | "cosine_warm_restarts" | "onecycle" | "step" | "multistep" | "plateau"

# è®¾å¤‡è®¾ç½®
USE_CUDA = True                  # æ˜¯å¦ä½¿ç”¨GPU

# æ—¥å¿—å’Œä¿å­˜
LOG_DIR = "./runs"                # TensorBoardæ—¥å¿—ç›®å½•
SAVE_MODEL = True                 # æ˜¯å¦ä¿å­˜æ¨¡å‹
SAVE_INTERVAL = 5                 # ä¿å­˜é—´éš”(epoch)

# æ•°æ®å¢å¼º
USE_AUGMENTATION = True           # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º

# å›¾åƒå°ºå¯¸è®¾ç½®
TARGET_SIZE = (256, 256)

# å¯é€‰ï¼šé™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°ä»¥ä¾¿å¿«é€Ÿå®éªŒ
MAX_TRAIN_BATCHES = None  # e.g., 30
MAX_VAL_BATCHES = None    # e.g., 30

# å­¦ä¹ ç‡è°ƒåº¦é¢å¤–å‚æ•°ï¼ˆç”¨äº step/multistep/plateauï¼‰
STEP_SIZE = 20
GAMMA = 0.1
MILESTONES = [60, 120, 160]
PLATEAU_PATIENCE = 10
PLATEAU_FACTOR = 0.1
PLATEAU_MIN_LR = 1e-6

# ==================== è¯„ä¼°æŒ‡æ ‡å‡½æ•° ====================

def calculate_iou(pred, target, n_classes=21, ignore_index=255):
    """
    è®¡ç®—IoUï¼ˆäº¤å¹¶æ¯”ï¼‰ - ä¿®å¤ç‰ˆæœ¬
    æ­£ç¡®å¤„ç†ignore_index
    """
    ious = []
    pred = torch.argmax(pred, dim=1)
    
    # åˆ›å»ºæœ‰æ•ˆåƒç´ çš„mask
    valid_mask = (target != ignore_index)
    
    for cls in range(n_classes):
        pred_inds = (pred == cls) & valid_mask
        target_inds = (target == cls) & valid_mask
        
        intersection = (pred_inds & target_inds).sum().float()
        union = (pred_inds | target_inds).sum().float()
        
        if union == 0:
            ious.append(float('nan'))  # æ²¡æœ‰è¯¥ç±»åˆ«ï¼Œè¿”å›NaN
        else:
            ious.append((intersection / union).item())
    
    return np.nanmean(ious)  # å¿½ç•¥NaNè®¡ç®—å‡å€¼

def calculate_dice(pred, target, n_classes=21, ignore_index=255):
    """
    è®¡ç®—Diceç³»æ•° - ä¿®å¤ç‰ˆæœ¬
    æ­£ç¡®å¤„ç†ignore_index
    """
    dices = []
    pred = torch.argmax(pred, dim=1)
    
    # åˆ›å»ºæœ‰æ•ˆåƒç´ çš„mask
    valid_mask = (target != ignore_index)
    
    for cls in range(n_classes):
        pred_inds = (pred == cls) & valid_mask
        target_inds = (target == cls) & valid_mask
        
        intersection = (pred_inds & target_inds).sum().float()
        total = pred_inds.sum().float() + target_inds.sum().float()
        
        if total == 0:
            dices.append(float('nan'))
        else:
            dices.append((2 * intersection / total).item())
    
    return np.nanmean(dices)

def calculate_pixel_accuracy(pred, target, ignore_index=255):
    """è®¡ç®—åƒç´ ç²¾åº¦"""
    pred = torch.argmax(pred, dim=1)
    valid_pixels = (target != ignore_index)
    
    if valid_pixels.sum() == 0:
        return 0.0
    
    correct_pixels = (pred[valid_pixels] == target[valid_pixels]).sum().float()
    total_pixels = valid_pixels.sum().float()
    
    return (correct_pixels / total_pixels).item()

# ==================== è®­ç»ƒå‡½æ•° ====================

def dice_loss(pred, target, smooth=1.0, ignore_index=255):
    """
    DiceæŸå¤±å‡½æ•° - ä¿®å¤ç‰ˆæœ¬
    æ”¯æŒignore_indexï¼Œæ­£ç¡®å¤„ç†VOCæ•°æ®é›†ä¸­çš„255å€¼
    """
    # åº”ç”¨softmaxè·å¾—æ¦‚ç‡åˆ†å¸ƒ
    pred = torch.softmax(pred, dim=1)
    
    # åˆ›å»ºæœ‰æ•ˆåƒç´ çš„maskï¼ˆæ’é™¤ignore_indexï¼‰
    valid_mask = (target != ignore_index)
    
    # å°†ignore_indexçš„å€¼ä¸´æ—¶è®¾ä¸º0ï¼Œé¿å…one_hotç¼–ç å‡ºé”™
    target_masked = target.clone()
    target_masked[~valid_mask] = 0
    
    # ç¡®ä¿target_maskedæ˜¯é•¿æ•´å‹
    target_masked = target_masked.long()
    
    # åˆ›å»ºone-hotç¼–ç 
    target_one_hot = torch.nn.functional.one_hot(target_masked, num_classes=N_CLASSES).permute(0, 3, 1, 2).float()
    
    # å°†æ— æ•ˆåƒç´ çš„one-hotç¼–ç è®¾ä¸º0
    valid_mask_expanded = valid_mask.unsqueeze(1).expand_as(target_one_hot)
    target_one_hot = target_one_hot * valid_mask_expanded.float()
    
    # åŒæ ·å°†é¢„æµ‹å€¼ä¸­å¯¹åº”æ— æ•ˆåƒç´ çš„éƒ¨åˆ†è®¾ä¸º0
    pred_masked = pred * valid_mask.unsqueeze(1).expand_as(pred).float()
    
    # è®¡ç®—Diceç³»æ•°
    intersection = torch.sum(pred_masked * target_one_hot, dim=(2, 3))
    union = torch.sum(pred_masked, dim=(2, 3)) + torch.sum(target_one_hot, dim=(2, 3))
    
    dice = (2. * intersection + smooth) / (union + smooth)
    return 1 - dice.mean()

def create_loss_function(loss_type):
    """åˆ›å»ºæŸå¤±å‡½æ•°"""
    if loss_type == "dice":
        def dice_ce_loss(pred, target):
            # çº¯ Dice è·¯çº¿ä¸æ··å…¥ CEï¼›ä¸ºä¿æŒ general è„šæœ¬å¯é€‰æ‹©ï¼Œæ­¤å¤„æä¾› dice è·¯çº¿
            dice_val = dice_loss(pred, target, ignore_index=255)
            return dice_val
        return dice_ce_loss
    elif loss_type == "combined":
        def combined_loss(pred, target):
            ce_loss = nn.CrossEntropyLoss(ignore_index=255)(pred, target)
            dice_val = dice_loss(pred, target, ignore_index=255)
            return 0.5 * ce_loss + 0.5 * dice_val
        return combined_loss
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_type}")

def create_optimizer(model, optimizer_type, lr, weight_decay):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    if optimizer_type == "adam":
        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type == "sgd":
        return optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    elif optimizer_type == "rmsprop":
        return optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

def print_config():
    """æ‰“å°å½“å‰é…ç½®"""
    print("=" * 60)
    print("è®­ç»ƒé…ç½®å‚æ•°:")
    print("=" * 60)
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"æŸå¤±å‡½æ•°: {LOSS_TYPE}")
    print(f"ä¼˜åŒ–å™¨: {OPTIMIZER_TYPE}")
    print(f"æ¨¡å‹ç±»å‹: {MODEL_TYPE}")
    print(f"è°ƒåº¦å™¨: {SCHEDULER_TYPE}")
    print(f"ä½¿ç”¨GPU: {USE_CUDA and torch.cuda.is_available()}")
    print(f"æ•°æ®å¢å¼º: {USE_AUGMENTATION}")
    print(f"ç›®æ ‡å°ºå¯¸: {TARGET_SIZE}")
    print("=" * 60)

def train_model():
    """è®­ç»ƒU-Netæ¨¡å‹"""
    
    # æ‰“å°é…ç½®
    print_config()
    
     # ä¿®å¤è®¾å¤‡è®¾ç½®é€»è¾‘ï¼
    USE_CUDA = True  # ç¡®ä¿è¿™ä¸ªå˜é‡æ˜¯True
    if torch.cuda.is_available() and USE_CUDA:
        device = torch.device('cuda')
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ”¢ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        device = torch.device('cpu')
        print("âš ï¸  ä½¿ç”¨CPUè®­ç»ƒ - æ£€æŸ¥CUDAé…ç½®")
    
    print(f"æœ€ç»ˆè®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    try:
        train_dataset = VOCSegmentationDataset(
            root_dir=DATA_DIR,
            split='train',
            apply_augmentation=USE_AUGMENTATION,
            target_size=TARGET_SIZE
        )
        
        val_dataset = VOCSegmentationDataset(
            root_dir=DATA_DIR,
            split='val',
            apply_augmentation=False,
            target_size=TARGET_SIZE
        )
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} å›¾åƒ")
        print(f"éªŒè¯é›†: {len(val_dataset)} å›¾åƒ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=False
    )
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    if MODEL_TYPE == "unet":
        model = UNet(n_channels=3, n_classes=N_CLASSES).to(device)
    else:
        model = SimpleUNet(n_channels=3, n_classes=N_CLASSES).to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f}M")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = create_loss_function(LOSS_TYPE)
    optimizer = create_optimizer(model, OPTIMIZER_TYPE, LEARNING_RATE, WEIGHT_DECAY)
    if SCHEDULER_TYPE == "cosine_warm_restarts":
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)
    elif SCHEDULER_TYPE == "cosine":
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)
    elif SCHEDULER_TYPE == "onecycle":
        steps_per_epoch = max(1, len(train_loader))
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=LEARNING_RATE,
            epochs=EPOCHS,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.1,
            anneal_strategy='cos',
            div_factor=10.0,
            final_div_factor=100.0
        )
    elif SCHEDULER_TYPE == "step":
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
    elif SCHEDULER_TYPE == "multistep":
        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)
    elif SCHEDULER_TYPE == "plateau":
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=PLATEAU_FACTOR, patience=PLATEAU_PATIENCE, min_lr=PLATEAU_MIN_LR)
    else:
        scheduler = None
    
    # TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    log_dir = os.path.join(LOG_DIR, f"unet_voc_{timestamp}")
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    print(f"TensorBoardæ—¥å¿—ç›®å½•: {log_dir}")
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    best_iou = 0.0
    train_losses = []
    val_losses = []
    val_ious = []
    val_dices = []
    val_accuracies = []
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("-" * 80)
    
    start_time = time.time()
    
    try:
        for epoch in range(EPOCHS):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_train_loss = 0.0
            
            for batch_idx, (images, masks) in enumerate(train_loader):
                images, masks = images.to(device), masks.to(device)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, masks)
                loss.backward()
                # æ¢¯åº¦è£å‰ªï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                # OneCycleLRæŒ‰stepæ¨è¿›
                if scheduler is not None and SCHEDULER_TYPE == "onecycle":
                    scheduler.step()
                
                epoch_train_loss += loss.item()
                
                if (batch_idx + 1) % 50 == 0:
                    print(f'Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')
                # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡æ•°ç”¨äºå¿«é€Ÿå®éªŒ
                if MAX_TRAIN_BATCHES is not None and (batch_idx + 1) >= MAX_TRAIN_BATCHES:
                    break
            
            avg_train_loss = epoch_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            epoch_val_loss = 0.0
            epoch_iou = 0.0
            epoch_dice = 0.0
            epoch_accuracy = 0.0
            
            with torch.no_grad():
                for batch_idx, (images, masks) in enumerate(val_loader):
                    images, masks = images.to(device), masks.to(device)
                    outputs = model(images)
                    
                    # è®¡ç®—æŸå¤±
                    loss = criterion(outputs, masks)
                    epoch_val_loss += loss.item()
                    
                    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                    epoch_iou += calculate_iou(outputs, masks, N_CLASSES)
                    epoch_dice += calculate_dice(outputs, masks, N_CLASSES)
                    epoch_accuracy += calculate_pixel_accuracy(outputs, masks)
                    # é™åˆ¶éªŒè¯æ‰¹æ¬¡æ•°ç”¨äºå¿«é€Ÿå®éªŒ
                    if MAX_VAL_BATCHES is not None and (batch_idx + 1) >= MAX_VAL_BATCHES:
                        break
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_val_loss = epoch_val_loss / len(val_loader)
            avg_iou = epoch_iou / len(val_loader)
            avg_dice = epoch_dice / len(val_loader)
            avg_accuracy = epoch_accuracy / len(val_loader)
            
            val_losses.append(avg_val_loss)
            val_ious.append(avg_iou)
            val_dices.append(avg_dice)
            val_accuracies.append(avg_accuracy)
            
            # å­¦ä¹ ç‡è°ƒæ•´ï¼ˆéOneCycleæŒ‰epochæ¨è¿›ï¼‰
            if scheduler is not None and SCHEDULER_TYPE != "onecycle":
                if SCHEDULER_TYPE == "cosine_warm_restarts":
                    scheduler.step(epoch + 1)
                elif SCHEDULER_TYPE == "plateau":
                    scheduler.step(avg_val_loss)
                else:
                    scheduler.step()
            
            # è®°å½•åˆ°TensorBoard
            writer.add_scalar('Loss/train', avg_train_loss, epoch)
            writer.add_scalar('Loss/val', avg_val_loss, epoch)
            writer.add_scalar('Metrics/IoU', avg_iou, epoch)
            writer.add_scalar('Metrics/Dice', avg_dice, epoch)
            writer.add_scalar('Metrics/Accuracy', avg_accuracy, epoch)
            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss and SAVE_MODEL:
                best_val_loss = avg_val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'val_iou': avg_iou,
                    'val_dice': avg_dice,
                    'val_accuracy': avg_accuracy,
                    'config': {
                        'batch_size': BATCH_SIZE,
                        'learning_rate': LEARNING_RATE,
                        'loss_type': LOSS_TYPE
                    }
                }, os.path.join(log_dir, 'best_model.pth'))
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")
            
            if avg_iou > best_iou:
                best_iou = avg_iou
            
            # å®šæœŸä¿å­˜
            if SAVE_MODEL and (epoch + 1) % SAVE_INTERVAL == 0:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }, os.path.join(log_dir, f'model_epoch_{epoch+1}.pth'))
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f'Epoch {epoch+1}/{EPOCHS}')
            print(f'Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')
            print(f'IoU: {avg_iou:.4f} | Dice: {avg_dice:.4f} | Accuracy: {avg_accuracy:.4f}')
            print(f'LR: {optimizer.param_groups[0]["lr"]:.2e}')
            print("-" * 80)
            
            # æ¸…ç†GPUç¼“å­˜
            if device.type == 'cuda':
                torch.cuda.empty_cache()
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if SAVE_MODEL:
        torch.save({
            'epoch': EPOCHS,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_ious': val_ious,
            'val_dices': val_dices,
            'val_accuracies': val_accuracies,
        }, os.path.join(log_dir, 'final_model.pth'))
    
    # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
    total_time = time.time() - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = int(total_time % 60)
    
    writer.close()
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"â° æ€»è®­ç»ƒæ—¶é—´: {hours}h {minutes}m {seconds}s")
    print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"ğŸ¯ æœ€ä½³IoU: {best_iou:.4f}")
    print(f"ğŸ’¾ æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜åœ¨: {log_dir}")
    print(f"ğŸ“ˆ å¯åŠ¨TensorBoard: tensorboard --logdir={LOG_DIR}")
    
    # ä¿å­˜è®­ç»ƒç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
    with open(os.path.join(log_dir, 'training_results.txt'), 'w') as f:
        f.write("è®­ç»ƒç»“æœæ€»ç»“:\n")
        f.write("=" * 50 + "\n")
        f.write(f"æ€»è®­ç»ƒæ—¶é—´: {hours}h {minutes}m {seconds}s\n")
        f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}\n")
        f.write(f"æœ€ä½³IoU: {best_iou:.4f}\n")
        f.write(f"æœ€ç»ˆå­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}\n")
        f.write("\næ¯ä¸ªepochçš„æŒ‡æ ‡:\n")
        for i, (tl, vl, iou, dice, acc) in enumerate(zip(train_losses, val_losses, val_ious, val_dices, val_accuracies)):
            f.write(f"Epoch {i+1}: Loss={tl:.4f}/{vl:.4f}, IoU={iou:.4f}, Dice={dice:.4f}, Acc={acc:.4f}\n")

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Train UNet on VOC2012 (loss: combined or dice)")
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°è¦†ç›–å€¼')
    parser.add_argument('--lr', type=float, help='å­¦ä¹ ç‡è¦†ç›–å€¼')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°è¦†ç›–å€¼')
    parser.add_argument('--data-dir', type=str, help='VOC2012 æ•°æ®ç›®å½•è¦†ç›–å€¼')
    parser.add_argument('--scheduler', type=str, choices=['cosine','cosine_warm_restarts','onecycle','step','multistep','plateau'], help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    parser.add_argument('--model', type=str, choices=['unet','simple'], help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--loss', type=str, choices=['combined','dice'], help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--no-aug', action='store_true', help='ç¦ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--max-train-batches', type=int, help='æ¯ä¸ªepochçš„è®­ç»ƒæ‰¹æ¬¡æ•°ä¸Šé™')
    parser.add_argument('--max-val-batches', type=int, help='æ¯ä¸ªepochçš„éªŒè¯æ‰¹æ¬¡æ•°ä¸Šé™')
    # è°ƒåº¦å™¨å‚æ•°
    parser.add_argument('--step-size', type=int, help='StepLR çš„ step_size')
    parser.add_argument('--gamma', type=float, help='StepLR/MultiStepLR çš„ gamma è¡°å‡å› å­')
    parser.add_argument('--milestones', type=str, help='MultiStepLR çš„é‡Œç¨‹ç¢‘ï¼Œä¾‹å¦‚ 60,120,160')
    parser.add_argument('--plateau-patience', type=int, help='ReduceLROnPlateau çš„ patience')
    parser.add_argument('--plateau-factor', type=float, help='ReduceLROnPlateau çš„ factor')
    parser.add_argument('--plateau-min-lr', type=float, help='ReduceLROnPlateau çš„ min_lr')
    args = parser.parse_args()

    # è¦†ç›–å…¨å±€é…ç½®ï¼ˆå¦‚æä¾›ï¼‰
    if args.epochs is not None:
        EPOCHS = args.epochs
    if args.lr is not None:
        LEARNING_RATE = args.lr
    if args.batch_size is not None:
        BATCH_SIZE = args.batch_size
    if args.data_dir is not None:
        DATA_DIR = args.data_dir
    if args.scheduler is not None:
        SCHEDULER_TYPE = args.scheduler
    if args.model is not None:
        MODEL_TYPE = args.model
    if hasattr(args, 'loss') and args.loss is not None:
        LOSS_TYPE = args.loss
    if args.no_aug:
        USE_AUGMENTATION = False
    if args.max_train_batches is not None:
        MAX_TRAIN_BATCHES = args.max_train_batches
    if args.max_val_batches is not None:
        MAX_VAL_BATCHES = args.max_val_batches
    # è¦†ç›–è°ƒåº¦å™¨å‚æ•°
    if hasattr(args, 'step_size') and args.step_size is not None:
        STEP_SIZE = args.step_size
    if hasattr(args, 'gamma') and args.gamma is not None:
        GAMMA = args.gamma
    if hasattr(args, 'milestones') and args.milestones:
        try:
            MILESTONES = [int(x.strip()) for x in args.milestones.split(',') if x.strip()]
        except Exception:
            print("è­¦å‘Š: è§£æ --milestones å¤±è´¥ï¼Œä¿æŒé»˜è®¤å€¼:", MILESTONES)
    if hasattr(args, 'plateau_patience') and args.plateau_patience is not None:
        PLATEAU_PATIENCE = args.plateau_patience
    if hasattr(args, 'plateau_factor') and args.plateau_factor is not None:
        PLATEAU_FACTOR = args.plateau_factor
    if hasattr(args, 'plateau_min_lr') and args.plateau_min_lr is not None:
        PLATEAU_MIN_LR = args.plateau_min_lr

    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    if not os.path.exists(DATA_DIR):
        print("=" * 80)
        print("âŒ é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨!")
        print("=" * 80)
        print(f"æœŸæœ›è·¯å¾„: {os.path.abspath(DATA_DIR)}")
        print()
        print("è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å‡†å¤‡PASCAL VOC 2012æ•°æ®é›†:")
        print("1. ä»å®˜æ–¹ç½‘ç«™ä¸‹è½½VOC2012æ•°æ®é›†:")
        print("   http://host.robots.ox.ac.uk/pascal/VOC/voc2012/")
        print()
        print("2. ä¸‹è½½å¹¶è§£å‹ä»¥ä¸‹æ–‡ä»¶:")
        print("   - VOCtrainval_11-May-2012.tar (è®­ç»ƒå’ŒéªŒè¯æ•°æ®)")
        print()
        print("3. å°†è§£å‹åçš„VOC2012æ–‡ä»¶å¤¹æ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œç¡®ä¿å…·æœ‰ä»¥ä¸‹ç»“æ„:")
        print("   ./VOC2012/")
        print("   â”œâ”€â”€ JPEGImages/          # åŸå§‹å›¾åƒ")
        print("   â”œâ”€â”€ SegmentationClass/   # åˆ†å‰²æ ‡æ³¨")
        print("   â””â”€â”€ ImageSets/")
        print("       â””â”€â”€ Segmentation/    # æ•°æ®åˆ†å‰²æ–‡ä»¶")
        print("           â”œâ”€â”€ train.txt")
        print("           â”œâ”€â”€ val.txt")
        print("           â””â”€â”€ trainval.txt")
        print()
        print("4. æˆ–è€…ä¿®æ”¹train_unet_voc.pyä¸­çš„DATA_DIRå˜é‡æŒ‡å‘æ­£ç¡®çš„æ•°æ®é›†è·¯å¾„")
        print("=" * 80)
        print()
        print("ğŸ’¡ æç¤º: å¦‚æœæ‚¨æƒ³è¦å¿«é€Ÿæµ‹è¯•ä»£ç ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªå°å‹æ¼”ç¤ºæ•°æ®é›†")
        print("   æˆ–ä½¿ç”¨å…¶ä»–å…¬å¼€å¯ç”¨çš„è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ã€‚")
        print("=" * 80)
    else:
        train_model()