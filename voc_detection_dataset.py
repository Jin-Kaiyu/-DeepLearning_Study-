"""
PASCAL VOC ç›®æ ‡æ£€æµ‹æ•°æ®é›†è‡ªå®šä¹‰Datasetç±»
å®ç°å›¾åƒå’Œè¾¹ç•Œæ¡†æ ‡æ³¨çš„åŠ è½½ï¼Œç”¨äºFaster R-CNNè®­ç»ƒ

ä½œè€…: DeepLearning_Study
æ—¥æœŸ: 2025å¹´9æœˆ
è¯¾ç¨‹: å·¥ä¸šè¡¨é¢åˆ’ç—•æ£€æµ‹ç ”ç©¶ - ç¬¬5å‘¨
"""

import os
import xml.etree.ElementTree as ET
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset
import torchvision.transforms as T
import torchvision.transforms.functional as TF
from typing import List, Dict, Any, Optional


class VOCDetectionDataset(Dataset):
    """
    PASCAL VOC ç›®æ ‡æ£€æµ‹æ•°æ®é›†ç±»
    
    æ•°æ®é›†ç»“æ„:
    VOC2012/
    â”œâ”€â”€ JPEGImages/          # åŸå§‹å›¾åƒ
    â”œâ”€â”€ Annotations/         # XMLæ ‡æ³¨æ–‡ä»¶
    â”œâ”€â”€ ImageSets/
    â”‚   â””â”€â”€ Main/
    â”‚       â”œâ”€â”€ train.txt    # è®­ç»ƒé›†å›¾åƒåç§°åˆ—è¡¨
    â”‚       â””â”€â”€ val.txt      # éªŒè¯é›†å›¾åƒåç§°åˆ—è¡¨
    """
    
    # PASCAL VOC ç±»åˆ«å®šä¹‰ï¼ˆ20ä¸ªç‰©ä½“ç±»åˆ«ï¼‰
    CLASSES = [
        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
        'bus', 'car', 'cat', 'chair', 'cow',
        'diningtable', 'dog', 'horse', 'motorbike', 'person',
        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
    ]
    
    def __init__(self, 
                 root_dir: str,
                 split: str = 'train',
                 transform: Optional[T.Compose] = None,
                 target_transform: Optional[Any] = None):
        """
        åˆå§‹åŒ–VOCæ£€æµ‹æ•°æ®é›†
        
        Args:
            root_dir: VOCæ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'trainval')
            transform: åº”ç”¨äºå›¾åƒçš„å˜æ¢
            target_transform: åº”ç”¨äºç›®æ ‡çš„å˜æ¢
        """
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.target_transform = target_transform
        
        # æ•°æ®è·¯å¾„
        self.images_dir = os.path.join(root_dir, 'JPEGImages')
        self.annotations_dir = os.path.join(root_dir, 'Annotations')
        self.splits_dir = os.path.join(root_dir, 'ImageSets', 'Main')
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        self._check_paths()
        
        # åŠ è½½å›¾åƒåˆ—è¡¨
        self.image_names = self._load_image_list()
        
        # åˆ›å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
        self.class_to_idx = {cls_name: idx + 1 for idx, cls_name in enumerate(self.CLASSES)}
        
        print(f"VOC Detection Dataset loaded: {len(self.image_names)} images in {split} split")
        print(f"Classes: {self.CLASSES}")
    
    def _check_paths(self):
        """æ£€æŸ¥å¿…è¦çš„è·¯å¾„æ˜¯å¦å­˜åœ¨"""
        required_paths = [self.images_dir, self.annotations_dir, self.splits_dir]
        for path in required_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Required path not found: {path}")
    
    def _load_image_list(self) -> List[str]:
        """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒåç§°åˆ—è¡¨"""
        split_file = os.path.join(self.splits_dir, f'{self.split}.txt')
        
        if not os.path.exists(split_file):
            raise FileNotFoundError(f"Split file not found: {split_file}")
        
        with open(split_file, 'r') as f:
            # å¤„ç†å¯èƒ½åŒ…å«æ ‡ç­¾ä¿¡æ¯çš„è¡Œï¼ˆå¦‚ "2007_000027 -1"ï¼‰
            lines = [line.strip().split()[0] for line in f.readlines()]
        
        return lines
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.image_names)
    
    def _parse_annotation(self, annotation_path: str) -> Dict[str, Any]:
        """è§£æXMLæ ‡æ³¨æ–‡ä»¶"""
        tree = ET.parse(annotation_path)
        root = tree.getroot()
        
        # è·å–å›¾åƒå°ºå¯¸
        size = root.find('size')
        width = int(size.find('width').text)
        height = int(size.find('height').text)
        
        boxes = []
        labels = []
        difficulties = []
        areas = []
        
        # è§£ææ¯ä¸ªç‰©ä½“
        for obj in root.findall('object'):
            # è·å–ç±»åˆ«
            cls_name = obj.find('name').text
            if cls_name not in self.class_to_idx:
                continue  # è·³è¿‡æœªçŸ¥ç±»åˆ«
            
            # è·å–è¾¹ç•Œæ¡†
            bndbox = obj.find('bndbox')
            xmin = float(bndbox.find('xmin').text)
            ymin = float(bndbox.find('ymin').text)
            xmax = float(bndbox.find('xmax').text)
            ymax = float(bndbox.find('ymax').text)
            
            # æ£€æŸ¥è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§
            if xmin >= xmax or ymin >= ymax:
                continue
            
            # è·å–éš¾åº¦æ ‡å¿—ï¼ˆå¦‚æœæœ‰ï¼‰
            difficult = obj.find('difficult')
            difficult = 0 if difficult is None else int(difficult.text)
            
            # è®¡ç®—é¢ç§¯
            area = (xmax - xmin) * (ymax - ymin)
            
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(self.class_to_idx[cls_name])
            difficulties.append(difficult)
            areas.append(area)
        
        if not boxes:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç‰©ä½“
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros(0, dtype=torch.int64)
            areas = torch.zeros(0, dtype=torch.float32)
            difficulties = torch.zeros(0, dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
            areas = torch.as_tensor(areas, dtype=torch.float32)
            difficulties = torch.as_tensor(difficulties, dtype=torch.int64)
        
        # æ„å»ºç›®æ ‡å­—å…¸
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([0]),  # å ä½ç¬¦ï¼Œä¼šåœ¨collate_fnä¸­æ›¿æ¢
            'area': areas,
            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64),
            'difficulties': difficulties
        }
        
        return target
    
    def __getitem__(self, idx: int):
        """
        è·å–æŒ‡å®šç´¢å¼•çš„æ•°æ®æ ·æœ¬
        
        Returns:
            image: å›¾åƒå¼ é‡ [C, H, W]
            target: åŒ…å«æ ‡æ³¨ä¿¡æ¯çš„å­—å…¸
        """
        # è·å–å›¾åƒåç§°
        img_name = self.image_names[idx]
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        img_path = os.path.join(self.images_dir, f'{img_name}.jpg')
        annotation_path = os.path.join(self.annotations_dir, f'{img_name}.xml')
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        # è§£ææ ‡æ³¨
        target = self._parse_annotation(annotation_path)
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        else:
            # é»˜è®¤å˜æ¢ï¼šè½¬æ¢ä¸ºå¼ é‡
            image = TF.to_tensor(image)
        
        # æ·»åŠ å›¾åƒID
        target['image_id'] = torch.tensor([idx])
        
        return image, target
    
    def collate_fn(self, batch):
        """
        è‡ªå®šä¹‰collateå‡½æ•°ï¼Œç”¨äºå¤„ç†ä¸åŒæ•°é‡çš„ç›®æ ‡
        """
        images = []
        targets = []
        
        for img, target in batch:
            images.append(img)
            targets.append(target)
        
        return images, targets
    
    def get_class_name(self, class_id: int) -> str:
        """æ ¹æ®ç±»åˆ«IDè·å–ç±»åˆ«åç§°"""
        if 1 <= class_id <= len(self.CLASSES):
            return self.CLASSES[class_id - 1]
        return 'background'
    
    def visualize_sample(self, idx: int) -> Dict[str, Any]:
        """
        å¯è§†åŒ–æŒ‡å®šæ ·æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        image, target = self.__getitem__(idx)
        
        return {
            'image': image,
            'target': target,
            'image_name': self.image_names[idx],
            'num_objects': len(target['boxes'])
        }


def create_voc_data_loaders(root_dir: str, 
                           batch_size: int = 2,
                           num_workers: int = 2):
    """
    åˆ›å»ºVOCæ•°æ®åŠ è½½å™¨
    
    Args:
        root_dir: VOCæ•°æ®é›†æ ¹ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
        
    Returns:
        train_loader, val_loader
    """
    # æ•°æ®å˜æ¢
    transform = T.Compose([
        T.ToTensor(),
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = VOCDetectionDataset(
        root_dir=root_dir,
        split='train',
        transform=transform
    )
    
    val_dataset = VOCDetectionDataset(
        root_dir=root_dir,
        split='val',
        transform=transform
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=train_dataset.collate_fn,
        pin_memory=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=val_dataset.collate_fn,
        pin_memory=True
    )
    
    return train_loader, val_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    root_dir = "VOC2012"  # æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹

    
    print("ğŸ§ª æµ‹è¯•VOCæ£€æµ‹æ•°æ®é›†...")
    
    try:
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        dataset = VOCDetectionDataset(root_dir, split='train')
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"ç±»åˆ«æ•°: {len(dataset.CLASSES)}")
        print(f"ç±»åˆ«æ˜ å°„: {dataset.class_to_idx}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        image, target = dataset[0]
        print(f"å›¾åƒå½¢çŠ¶: {image.shape}")
        print(f"ç›®æ ‡é”®å€¼: {list(target.keys())}")
        print(f"è¾¹ç•Œæ¡†æ•°é‡: {len(target['boxes'])}")
        print(f"æ ‡ç­¾: {target['labels']}")
        
        print("âœ… æ•°æ®é›†æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()