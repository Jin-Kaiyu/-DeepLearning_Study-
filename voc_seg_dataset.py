"""
PASCAL VOC 2012 è¯­ä¹‰åˆ†å‰²æ•°æ®é›†è‡ªå®šä¹‰Datasetç±»
å®ç°å›¾åƒå’Œåˆ†å‰²maskçš„åŠ è½½ï¼Œæ”¯æŒæ•°æ®å¢å¼ºä¸”ä¿æŒåŒæ­¥

ä½œè€…: Kayu J
æ—¥æœŸ: 2025å¹´9æœˆ16æ—¥
è¯¾ç¨‹: ç›®æ ‡æ£€æµ‹ä¸è¯­ä¹‰åˆ†å‰²
"""

import os
import numpy as np
from PIL import Image
from typing import Tuple, Optional, Dict, List
import random

# å°è¯•å¯¼å…¥PyTorchï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
try:
    import torch
    from torch.utils.data import Dataset
    import torchvision.transforms as transforms
    import torchvision.transforms.functional as TF
    TORCH_AVAILABLE = True
    print("âœ… PyTorchå·²å¯¼å…¥")
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    # å®šä¹‰ç®€å•çš„DatasetåŸºç±»
    class Dataset:
        def __init__(self):
            pass
        
        def __len__(self):
            raise NotImplementedError
        
        def __getitem__(self, idx):
            raise NotImplementedError

class VOCSegmentationDataset(Dataset):
    """
    PASCAL VOC 2012 è¯­ä¹‰åˆ†å‰²æ•°æ®é›†
    
    æ•°æ®é›†ç»“æ„:
    VOC2012/
    â”œâ”€â”€ JPEGImages/          # åŸå§‹å›¾åƒ
    â”œâ”€â”€ SegmentationClass/   # è¯­ä¹‰åˆ†å‰²æ ‡ç­¾(æ¯ä¸ªåƒç´ æ ‡æ³¨ç±»åˆ«)
    â”œâ”€â”€ SegmentationObject/  # å®ä¾‹åˆ†å‰²æ ‡ç­¾
    â””â”€â”€ ImageSets/
        â””â”€â”€ Segmentation/
            â”œâ”€â”€ train.txt    # è®­ç»ƒé›†å›¾åƒåç§°åˆ—è¡¨
            â”œâ”€â”€ val.txt      # éªŒè¯é›†å›¾åƒåç§°åˆ—è¡¨
            â””â”€â”€ trainval.txt # è®­ç»ƒ+éªŒè¯é›†
    """
    
    # PASCAL VOC 2012 ç±»åˆ«å®šä¹‰ï¼ˆ21ç±»ï¼š20ä¸ªç‰©ä½“ç±»åˆ« + 1ä¸ªèƒŒæ™¯ï¼‰
    CLASSES = [
        'background',       # 0
        'aeroplane',        # 1
        'bicycle',          # 2
        'bird',             # 3
        'boat',             # 4
        'bottle',           # 5
        'bus',              # 6
        'car',              # 7
        'cat',              # 8
        'chair',            # 9
        'cow',              # 10
        'diningtable',      # 11
        'dog',              # 12
        'horse',            # 13
        'motorbike',        # 14
        'person',           # 15
        'pottedplant',      # 16
        'sheep',            # 17
        'sofa',             # 18
        'train',            # 19
        'tvmonitor'         # 20
    ]
    
    # ç±»åˆ«å¯¹åº”çš„å¯è§†åŒ–é¢œè‰²(RGB)
    CLASS_COLORS = [
        (0, 0, 0),          # background - é»‘è‰²
        (128, 0, 0),        # aeroplane - æ·±çº¢
        (0, 128, 0),        # bicycle - ç»¿è‰²
        (128, 128, 0),      # bird - æ©„æ¦„è‰²
        (0, 0, 128),        # boat - è“è‰²
        (128, 0, 128),      # bottle - ç´«è‰²
        (0, 128, 128),      # bus - é’è‰²
        (128, 128, 128),    # car - ç°è‰²
        (64, 0, 0),         # cat - æš—çº¢
        (192, 0, 0),        # chair - äº®çº¢
        (64, 128, 0),       # cow - é»„ç»¿
        (192, 128, 0),      # diningtable - æ©™è‰²
        (64, 0, 128),       # dog - è“ç´«
        (192, 0, 128),      # horse - ç²‰çº¢
        (64, 128, 128),     # motorbike - è“ç»¿
        (192, 128, 128),    # person - æµ…ç°
        (0, 64, 0),         # pottedplant - æ·±ç»¿
        (128, 64, 0),       # sheep - æ£•è‰²
        (0, 192, 0),        # sofa - äº®ç»¿
        (128, 192, 0),      # train - é»„ç»¿2
        (0, 64, 128)        # tvmonitor - æ·±è“
    ]
    
    def __init__(self, 
                 root_dir: str,
                 split: str = 'train',
                 transform: Optional[object] = None,
                 target_transform: Optional[object] = None,
                 apply_augmentation: bool = True):
        """
        åˆå§‹åŒ–VOCåˆ†å‰²æ•°æ®é›†
        
        Args:
            root_dir: VOC2012æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'trainval')
            transform: åº”ç”¨äºå›¾åƒçš„å˜æ¢
            target_transform: åº”ç”¨äºmaskçš„å˜æ¢
            apply_augmentation: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        """
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.target_transform = target_transform
        self.apply_augmentation = apply_augmentation
        
        # æ•°æ®è·¯å¾„
        self.images_dir = os.path.join(root_dir, 'JPEGImages')
        self.masks_dir = os.path.join(root_dir, 'SegmentationClass')
        self.splits_dir = os.path.join(root_dir, 'ImageSets', 'Segmentation')
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        self._check_paths()
        
        # åŠ è½½å›¾åƒåˆ—è¡¨
        self.image_names = self._load_image_list()
        
        print(f"VOC Segmentation Dataset loaded: {len(self.image_names)} images in {split} split")
    
    def _check_paths(self):
        """æ£€æŸ¥å¿…è¦çš„è·¯å¾„æ˜¯å¦å­˜åœ¨"""
        required_paths = [self.images_dir, self.masks_dir, self.splits_dir]
        for path in required_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Required path not found: {path}")
    
    def _load_image_list(self) -> List[str]:
        """åŠ è½½æŒ‡å®šsplitçš„å›¾åƒåç§°åˆ—è¡¨"""
        split_file = os.path.join(self.splits_dir, f'{self.split}.txt')
        
        if not os.path.exists(split_file):
            raise FileNotFoundError(f"Split file not found: {split_file}")
        
        with open(split_file, 'r') as f:
            image_names = [line.strip() for line in f.readlines()]
        
        return image_names
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.image_names)
    
    def __getitem__(self, idx: int):
        """
        è·å–æŒ‡å®šç´¢å¼•çš„æ•°æ®æ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            å¦‚æœPyTorchå¯ç”¨: tuple: (image_tensor, mask_tensor)
            å¦‚æœPyTorchä¸å¯ç”¨: dict: {'image': numpy_array, 'mask': numpy_array, 'name': str}
        """
        # è·å–å›¾åƒåç§°
        img_name = self.image_names[idx]
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        img_path = os.path.join(self.images_dir, f'{img_name}.jpg')
        mask_path = os.path.join(self.masks_dir, f'{img_name}.png')
        
        # åŠ è½½å›¾åƒå’Œmask
        image = Image.open(img_path).convert('RGB')
        mask = Image.open(mask_path).convert('P')  # Pæ¨¡å¼ä¿æŒè°ƒè‰²æ¿
        
        # åº”ç”¨åŒæ­¥çš„æ•°æ®å¢å¼º
        if self.apply_augmentation and self.split == 'train' and TORCH_AVAILABLE:
            image, mask = self._apply_synchronized_augmentation(image, mask)
        
        if TORCH_AVAILABLE:
            # PyTorchç‰ˆæœ¬ï¼šè¿”å›å¼ é‡
            # åº”ç”¨å˜æ¢
            if self.transform:
                image = self.transform(image)
            else:
                # é»˜è®¤å˜æ¢ï¼šè½¬æ¢ä¸ºå¼ é‡å¹¶å½’ä¸€åŒ–
                image = TF.to_tensor(image)
                image = TF.normalize(image, mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
            
            if self.target_transform:
                mask = self.target_transform(mask)
            else:
                # é»˜è®¤å˜æ¢ï¼šè½¬æ¢ä¸ºå¼ é‡ï¼Œä¿æŒç±»åˆ«æ ‡ç­¾
                mask = torch.from_numpy(np.array(mask)).long()
            
            return image, mask
        else:
            # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›numpyæ•°ç»„å’Œå­—å…¸
            return {
                'name': img_name,
                'image': np.array(image),
                'mask': np.array(mask)
            }
    
    def _apply_synchronized_augmentation(self, image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:
        """
        åº”ç”¨åŒæ­¥çš„æ•°æ®å¢å¼ºï¼Œç¡®ä¿å›¾åƒå’Œmaskä¿æŒä¸€è‡´
        
        Args:
            image: PILå›¾åƒ
            mask: PIL mask
            
        Returns:
            å¢å¼ºåçš„å›¾åƒå’Œmask
        """
        if not TORCH_AVAILABLE:
            # å¦‚æœPyTorchä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸå›¾åƒ
            return image, mask
            
        # éšæœºæ°´å¹³ç¿»è½¬
        if random.random() > 0.5:
            image = TF.hflip(image)
            mask = TF.hflip(mask)
        
        # éšæœºæ—‹è½¬ (-10åº¦åˆ°10åº¦)
        if random.random() > 0.5:
            angle = random.uniform(-10, 10)
            image = TF.rotate(image, angle, fill=0)
            mask = TF.rotate(mask, angle, fill=255)  # 255æ˜¯ignore_index
        
        # éšæœºç¼©æ”¾å’Œè£å‰ª
        if random.random() > 0.5:
            # åŸå§‹å°ºå¯¸
            w, h = image.size
            
            # éšæœºç¼©æ”¾æ¯”ä¾‹ (0.8 - 1.2)
            scale = random.uniform(0.8, 1.2)
            new_w, new_h = int(w * scale), int(h * scale)
            
            # ç¼©æ”¾
            image = TF.resize(image, (new_h, new_w))
            mask = TF.resize(mask, (new_h, new_w), interpolation=Image.NEAREST)
            
            # å¦‚æœæ”¾å¤§äº†ï¼Œè¿›è¡Œéšæœºè£å‰ª
            if scale > 1.0:
                # éšæœºè£å‰ªå›åŸå§‹å°ºå¯¸
                i = random.randint(0, new_h - h)
                j = random.randint(0, new_w - w)
                image = TF.crop(image, i, j, h, w)
                mask = TF.crop(mask, i, j, h, w)
            else:
                # å¦‚æœç¼©å°äº†ï¼Œpaddingåˆ°åŸå§‹å°ºå¯¸
                pad_h = h - new_h
                pad_w = w - new_w
                padding = (pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2)
                image = TF.pad(image, padding, fill=0)
                mask = TF.pad(mask, padding, fill=255)
        
        return image, mask
    
    def get_class_weights(self):
        """
        è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
        
        Returns:
            å¦‚æœPyTorchå¯ç”¨: torch.Tensor
            å¦‚æœPyTorchä¸å¯ç”¨: numpy.ndarray
        """
        print("æ­£åœ¨è®¡ç®—ç±»åˆ«æƒé‡...")
        
        if TORCH_AVAILABLE:
            class_counts = torch.zeros(len(self.CLASSES))
        else:
            class_counts = np.zeros(len(self.CLASSES))
        
        for idx in range(len(self)):
            data = self.__getitem__(idx)
            
            if TORCH_AVAILABLE:
                _, mask = data
                unique, counts = torch.unique(mask, return_counts=True)
            else:
                mask = data['mask']
                unique, counts = np.unique(mask, return_counts=True)
            
            for class_id, count in zip(unique, counts):
                if class_id < len(self.CLASSES):  # å¿½ç•¥255ç­‰ç‰¹æ®Šå€¼
                    class_counts[class_id] += count
        
        # è®¡ç®—æƒé‡ (æ€»åƒç´ æ•° / (ç±»åˆ«æ•° * æ¯ä¸ªç±»åˆ«çš„åƒç´ æ•°))
        if TORCH_AVAILABLE:
            total_pixels = class_counts.sum()
            weights = total_pixels / (len(self.CLASSES) * class_counts)
            weights[class_counts == 0] = 0  # é¿å…é™¤é›¶
        else:
            total_pixels = class_counts.sum()
            weights = np.divide(total_pixels, (len(self.CLASSES) * class_counts), 
                               out=np.zeros_like(class_counts), where=class_counts!=0)
        
        return weights
    
    def visualize_sample(self, idx: int) -> Dict:
        """
        å¯è§†åŒ–æŒ‡å®šæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            åŒ…å«åŸå§‹å›¾åƒã€maskå’Œå åŠ å›¾åƒçš„å­—å…¸
        """
        img_name = self.image_names[idx]
        img_path = os.path.join(self.images_dir, f'{img_name}.jpg')
        mask_path = os.path.join(self.masks_dir, f'{img_name}.png')
        
        # åŠ è½½åŸå§‹å›¾åƒå’Œmask
        image = Image.open(img_path).convert('RGB')
        mask = Image.open(mask_path).convert('P')
        
        # è½¬æ¢maskä¸ºå½©è‰²
        mask_array = np.array(mask)
        colored_mask = np.zeros((*mask_array.shape, 3), dtype=np.uint8)
        
        for class_id, color in enumerate(self.CLASS_COLORS):
            colored_mask[mask_array == class_id] = color
        
        return {
            'image_name': img_name,
            'image': np.array(image),
            'mask': mask_array,
            'colored_mask': colored_mask,
            'classes_present': np.unique(mask_array)
        }
    
    @staticmethod
    def get_class_name(class_id: int) -> str:
        """æ ¹æ®ç±»åˆ«IDè·å–ç±»åˆ«åç§°"""
        if 0 <= class_id < len(VOCSegmentationDataset.CLASSES):
            return VOCSegmentationDataset.CLASSES[class_id]
        return 'unknown'
    
    def get_dataset_statistics(self) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("æ­£åœ¨è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
        
        stats = {
            'total_images': len(self.image_names),
            'split': self.split,
            'num_classes': len(self.CLASSES),
            'classes': self.CLASSES,
            'image_sizes': [],
            'class_distribution': np.zeros(len(self.CLASSES)) if not TORCH_AVAILABLE else torch.zeros(len(self.CLASSES))
        }
        
        # é‡‡æ ·éƒ¨åˆ†å›¾åƒè¿›è¡Œç»Ÿè®¡ï¼ˆé¿å…å¤ªæ…¢ï¼‰
        sample_size = min(100, len(self.image_names))
        sample_indices = random.sample(range(len(self.image_names)), sample_size)
        
        for idx in sample_indices:
            img_name = self.image_names[idx]
            img_path = os.path.join(self.images_dir, f'{img_name}.jpg')
            mask_path = os.path.join(self.masks_dir, f'{img_name}.png')
            
            # å›¾åƒå°ºå¯¸
            with Image.open(img_path) as img:
                stats['image_sizes'].append(img.size)
            
            # ç±»åˆ«åˆ†å¸ƒ
            with Image.open(mask_path) as mask:
                mask_array = np.array(mask)
                unique_classes = np.unique(mask_array)
                for class_id in unique_classes:
                    if class_id < len(self.CLASSES):
                        stats['class_distribution'][class_id] += 1
        
        return stats


def create_data_loaders(root_dir: str, 
                       batch_size: int = 4,
                       num_workers: int = 2,
                       pin_memory: bool = True):
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆä»…åœ¨PyTorchå¯ç”¨æ—¶ï¼‰
    
    Args:
        root_dir: VOC2012æ•°æ®é›†æ ¹ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
        pin_memory: æ˜¯å¦å›ºå®šå†…å­˜
        
    Returns:
        å¦‚æœPyTorchå¯ç”¨: (train_loader, val_loader)
        å¦‚æœPyTorchä¸å¯ç”¨: None
    """
    if not TORCH_AVAILABLE:
        print("âš ï¸ PyTorchä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºDataLoader")
        return None
        
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = VOCSegmentationDataset(
        root_dir=root_dir,
        split='train',
        apply_augmentation=True
    )
    
    val_dataset = VOCSegmentationDataset(
        root_dir=root_dir,
        split='val',
        apply_augmentation=False
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False
    )
    
    return train_loader, val_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    root_dir = r"VOC2012"
    
    print(f"ğŸ§ª æµ‹è¯•VOCåˆ†å‰²æ•°æ®é›†...")
    print(f"PyTorchå¯ç”¨: {TORCH_AVAILABLE}")
    
    try:
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        dataset = VOCSegmentationDataset(root_dir, split='train', apply_augmentation=False)
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"ç±»åˆ«æ•°: {len(dataset.CLASSES)}")
        print(f"ç±»åˆ«: {dataset.CLASSES[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªç±»åˆ«
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        data = dataset[0]
        
        if TORCH_AVAILABLE:
            image, mask = data
            print(f"å›¾åƒå½¢çŠ¶: {image.shape}")
            print(f"Maskå½¢çŠ¶: {mask.shape}")
            print(f"Maskä¸­çš„ç±»åˆ«: {torch.unique(mask)}")
        else:
            print(f"å›¾åƒåç§°: {data['name']}")
            print(f"å›¾åƒå½¢çŠ¶: {data['image'].shape}")
            print(f"Maskå½¢çŠ¶: {data['mask'].shape}")
            print(f"Maskä¸­çš„ç±»åˆ«: {np.unique(data['mask'])}")
        
        print("âœ… æ•°æ®é›†æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
